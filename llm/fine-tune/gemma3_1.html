<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Fine-Tuning Gemma 3 with Dolly Dataset</title>
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
  <link rel="icon" type="image/x-icon" href="https://mohan-chinnappan-n5.github.io/dfv/img/mc_favIcon.ico">
  <!-- Prism.js CSS for syntax highlighting -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
  <style>
    body {
      min-height: 100vh;
      display: flex;
      flex-direction: column;
    }
    main {
      flex: 1;
    }
    html {
      scroll-behavior: smooth;
    }
    img {
      border: 5px solid #ccc;
      border-radius: 8px;
      box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.3);
      margin: 10px;
      max-width: 100%;
      height: auto;
      display: block;
    }
    /* Style for copy button */
    .copy-btn {
      position: absolute;
      top: 0.5rem;
      right: 0.5rem;
      transition: background-color 0.2s;
    }
    .copy-btn:hover {
      background-color: #2563eb;
    }
    pre {
      position: relative;
      background-color: #f4f4f4;
      padding: 2rem 1rem 1rem;
      border-radius: 0.5rem;
    }
  </style>
</head>
<body class="bg-gray-100 font-sans">
  <!-- Sticky Navbar -->
  <nav class="bg-blue-600 text-white shadow-lg sticky top-0 z-50">
    <div class="max-w-6xl mx-auto px-4 py-4 flex justify-between items-center">
      <div class="text-2xl font-bold">
        <a href="#home">Fine-Tuning Gemma 3</a>
      </div>
      <div class="space-x-4">
        <a href="#home" class="hover:text-blue-200">Home</a>
        <a href="#features" class="hover:text-blue-200">Features</a>
        <a href="#techniques" class="hover:text-blue-200">Techniques</a>
        <a href="https://github.com/mchinnappan100/gemma3-finetuning" class="bg-blue-500 px-4 py-2 rounded hover:bg-blue-400">View on GitHub</a>
      </div>
    </div>
  </nav>

  <!-- Main Content -->
  <main>
    <!-- Hero Section -->
    <section id="home" class="bg-gradient-to-r from-blue-500 to-blue-700 text-white py-20">
      <div class="max-w-6xl mx-auto px-4 text-center">
        <h1 class="text-5xl font-bold mb-4">Fine-Tuning Gemma 3 with Dolly Dataset</h1>
        <p class="text-xl mb-8">Learn how to fine-tune Google's Gemma 3 model using LoRA and the Databricks Dolly dataset with Keras, optimized for efficiency and performance.</p>
        <a href="#techniques" class="bg-white text-blue-600 px-6 py-3 rounded-lg font-semibold hover:bg-gray-200">Explore Techniques</a>
      </div>
    </section>

    <!-- Features Section -->
    <section id="features" class="py-16">
      <div class="max-w-6xl mx-auto px-4">
        <h2 class="text-3xl font-bold text-gray-800 text-center mb-12">Features of Gemma 3 Fine-Tuning</h2>
        <div class="grid grid-cols-1 md:grid-cols-4 gap-8">
          <!-- Feature 1 -->
          <div class="bg-white p-6 rounded-lg shadow-md text-center">
            <div class="text-blue-600 text-4xl mb-4">‚ö°</div>
            <h3 class="text-xl font-semibold mb-2">Memory Efficiency</h3>
            <p class="text-gray-600">LoRA and quantization reduce memory usage, enabling fine-tuning on GPUs like Colab‚Äôs T4.</p>
          </div>
          <!-- Feature 2 -->
          <div class="bg-white p-6 rounded-lg shadow-md text-center">
            <div class="text-blue-600 text-4xl mb-4">üöÄ</div>
            <h3 class="text-xl font-semibold mb-2">Fast Training</h3>
            <p class="text-gray-600">Keras-based optimization with LoRA speeds up training while maintaining quality.</p>
          </div>
          <!-- Feature 3 -->
          <div class="bg-white p-6 rounded-lg shadow-md text-center">
            <div class="text-blue-600 text-4xl mb-4">üì¶</div>
            <h3 class="text-xl font-semibold mb-2">Compact Weights</h3>
            <p class="text-gray-600">LoRA produces small model weights (hundreds of MBs) for easy deployment.</p>
          </div>
          <!-- Feature 4 -->
          <div class="bg-white p-6 rounded-lg shadow-md text-center">
            <div class="text-blue-600 text-4xl mb-4">‚úÖ</div>
            <h3 class="text-xl font-semibold mb-2">High-Quality Outputs</h3>
            <p class="text-gray-600">Fine-tuned Gemma 3 delivers accurate responses for downstream tasks.</p>
          </div>
        </div>
      </div>
    </section>

    <!-- Fine-Tuning Techniques Section -->
    <section id="techniques" class="py-16">
      <div class="max-w-6xl mx-auto px-4">
        <h2 class="text-3xl font-bold text-gray-800 mb-8">Fine-Tuning Techniques for Gemma 3</h2>
        <p class="text-gray-600 mb-6">Explore the key techniques used to fine-tune Gemma 3 with the Databricks Dolly dataset and other methods:</p>
        <div class="space-y-6">
          <!-- Technique 1: LoRA -->
          <div class="bg-white p-6 rounded-lg shadow-md">
            <h3 class="text-xl font-semibold text-gray-800 mb-2">üîπ Low Rank Adaptation (LoRA)</h3>
            <p class="text-gray-600 mb-4">Low Rank Adaptation (LoRA) is a fine-tuning technique that reduces the number of trainable parameters by freezing the model‚Äôs weights and adding a small number of new weights. Using the <code>keras-hub</code> library, LoRA is applied to fine-tune Gemma 3 on the Databricks Dolly dataset, making training faster and memory-efficient while producing compact model weights.</p>
            <p class="text-gray-600 mb-4">Example LoRA fine-tuning with Dolly dataset:</p>
            <pre>
              <code class="language-python">
import keras
import keras_hub
import json

# Load model
gemma_lm = keras_hub.models.Gemma3CausalLM.from_preset("gemma3_instruct_1b")

# Load Dolly dataset
!wget -O databricks-dolly-15k.jsonl https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl

prompts = []
responses = []
line_count = 0
with open('/content/databricks-dolly-15k.jsonl') as file:
    for line in file:
        if line_count >= 1000:
            break
        examples = json.loads(line)
        if examples["context"]:
            continue
        prompts.append(examples["instruction"])
        responses.append(examples["response"])
        line_count += 1

data = {"prompts": prompts, "responses": responses}

# Enable LoRA
gemma_lm.backbone.enable_lora(rank=4)

# Configure training
gemma_lm.preprocessor.sequence_length = 256
optimizer = keras.optimizers.AdamW(learning_rate=5e-5, weight_decay=0.01)
optimizer.exclude_from_weight_decay(var_names=["bias", "scale"])
gemma_lm.compile(
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=optimizer,
    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()]
)

# Fine-tune
gemma_lm.fit(data, epochs=1, batch_size=1)
              </code>
              <button class="copy-btn bg-blue-600 text-white px-2 py-1 rounded text-sm" onclick="copyCode(this)">Copy</button>
            </pre>
          </div>
          <!-- Technique 2: 4-bit Quantization -->
          <div class="bg-white p-6 rounded-lg shadow-md">
            <h3 class="text-xl font-semibold text-gray-800 mb-2">üîπ 4-bit Quantization</h3>
            <p class="text-gray-600 mb-4">4-bit quantization, implemented via the <code>bitsandbytes</code> library, reduces memory usage by representing weights in 4-bit precision. This enables fine-tuning Gemma 3 on GPUs with limited memory, such as Colab‚Äôs T4.</p>
            <p class="text-gray-600 mb-4">Example quantization setup (alternative to Keras):</p>
            <pre>
              <code class="language-python">
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-3-4b-it",
    device_map="auto",
    torch_dtype=torch.bfloat16,
    load_in_4bit=True,
    quantization_config={"bnb_4bit_compute_dtype": torch.bfloat16}
)
              </code>
              <button class="copy-btn bg-blue-600 text-white px-2 py-1 rounded text-sm" onclick="copyCode(this)">Copy</button>
            </pre>
          </div>
          <!-- Technique 3: Supervised Fine-Tuning (SFT) -->
          <div class="bg-white p-6 rounded-lg shadow-md">
            <h3 class="text-xl font-semibold text-gray-800 mb-2">üîπ Supervised Fine-Tuning (SFT)</h3>
            <p class="text-gray-600 mb-4">Supervised Fine-Tuning (SFT) adapts Gemma 3 to specific tasks using datasets like Dolly. The Keras-based approach uses <code>SFTTrainer</code> or <code>fit</code> to train on instruction-response pairs.</p>
            <p class="text-gray-600 mb-4">Example SFT setup with Transformers (alternative):</p>
            <pre>
              <code class="language-python">
from trl import SFTTrainer
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./gemma3_finetuned",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    save_steps=100,
    save_total_limit=2,
    optim="adamw_torch"
)

trainer = SFTTrainer(
    model=model,
    train_dataset=tokenized_dataset,
    args=training_args
)
trainer.train()
              </code>
              <button class="copy-btn bg-blue-600 text-white px-2 py-1 rounded text-sm" onclick="copyCode(this)">Copy</button>
            </pre>
          </div>
          <!-- Technique 4: Gradient Checkpointing -->
          <div class="bg-white p-6 rounded-lg shadow-md">
            <h3 class="text-xl font-semibold text-gray-800 mb-2">üîπ Gradient Checkpointing</h3>
            <p class="text-gray-600 mb-4">Gradient checkpointing reduces memory usage by recomputing intermediate activations, enabling fine-tuning of large models like Gemma 3 on limited hardware.</p>
            <p class="text-gray-600 mb-4">Example setup (alternative to Keras):</p>
            <pre>
              <code class="language-python">
model.gradient_checkpointing_enable()
training_args = TrainingArguments(
    ...,
    gradient_checkpointing=True
)
              </code>
              <button class="copy-btn bg-blue-600 text-white px-2 py-1 rounded text-sm" onclick="copyCode(this)">Copy</button>
            </pre>
          </div>
          <!-- Technique 5: Mixed Precision Training -->
          <div class="bg-white p-6 rounded-lg shadow-md">
            <h3 class="text-xl font-semibold text-gray-800 mb-2">üîπ Mixed Precision Training</h3>
            <p class="text-gray-600 mb-4">Mixed precision training uses 16-bit floating-point (FP16 or BF16) to speed up training and reduce memory usage. The Dolly fine-tuning example uses <code>fp16=True</code>.</p>
            <p class="text-gray-600 mb-4">Example setup:</p>
            <pre>
              <code class="language-python">
training_args = TrainingArguments(
    ...,
    fp16=True  # Enable mixed precision
)
              </code>
              <button class="copy-btn bg-blue-600 text-white px-2 py-1 rounded text-sm" onclick="copyCode(this)">Copy</button>
            </pre>
          </div>
        </div>
        <div class="mt-8">
          <h3 class="text-xl font-semibold text-gray-800 mb-4">Prerequisites</h3>
          <p class="text-gray-600">Ensure the following are installed and configured:</p>
          <ul class="list-disc list-inside text-gray-600">
            <li>Python 3.8+ with libraries: <code>keras</code>, <code>keras-hub</code>, <code>keras-nlp</code>, <code>torch</code>, <code>transformers</code>, <code>datasets</code>, <code>peft</code>, <code>trl</code>, <code>accelerate</code>, <code>bitsandbytes</code>.</li>
            <li>Kaggle API credentials for downloading the Dolly dataset.</li>
            <li>A GPU (e.g., NVIDIA T4 in Google Colab) for efficient training.</li>
            <li>Access to the <code>gemma3_instruct_1b</code> model (requires authentication).</li>
            <li>The <code>databricks-dolly-15k</code> dataset from Hugging Face.</li>
          </ul>
        </div>
      </div>
    </section>
  </main>

  <!-- Footer -->
  <footer class="bg-blue-600 text-white py-6">
    <div class="max-w-6xl mx-auto px-4 text-center">
      <p>Made with ‚ù§Ô∏è in <a href="https://en.wikipedia.org/wiki/New_Hampshire" class="text-blue-200 hover:underline">New Hampshire</a> by <a href="https://mohan-chinnappan-n.github.io/about/cv.html" class="text-blue-200 hover:underline">mc</a></p>
    </div>
  </footer>

  <!-- Prism.js for syntax highlighting -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
  <!-- Clipboard copy script -->
  <script>
    function copyCode(button) {
      const code = button.previousElementSibling.textContent;
      navigator.clipboard.writeText(code.trim()).then(() => {
        button.textContent = 'Copied!';
        button.classList.add('bg-green-600');
        setTimeout(() => {
          button.textContent = 'Copy';
          button.classList.remove('bg-green-600');
        }, 2000);
      }).catch(err => {
        console.error('Failed to copy:', err);
        button.textContent = 'Error';
        button.classList.add('bg-red-600');
        setTimeout(() => {
          button.textContent = 'Copy';
          button.classList.remove('bg-red-600');
        }, 2000);
      });
    }
  </script>
</body>
</html>